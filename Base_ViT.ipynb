{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2325f5ea",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "TensorFlow 2.4 or higher TensorFlow Addons can be installed using :\n",
    "pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0efd0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5af3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-addons) (23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7c0edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f4976",
   "metadata": {},
   "source": [
    "##Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb5159",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a91556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of classes in the dataset\n",
    "num_classes = 100\n",
    "\n",
    "# Define the input shape for the dataset, which is a 32x32 image with 3 color channels (RGB)\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the CIFAR-100 dataset from Keras datasets, splitting it into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a267713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the training data (x_train and y_train)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "\n",
    "# x_train shape: (number of training samples, height, width, color channels)\n",
    "\n",
    "# Display the shape of the testing data (x_test and y_test)\n",
    "\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a35baa",
   "metadata": {},
   "source": [
    "## Declaring Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf18204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate controls the step size during model training\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Weight decay is a regularization term to prevent overfitting\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Batch size defines the number of samples processed in each iteration of training\n",
    "batch_size = 256\n",
    "\n",
    "# Number of training epochs specifies how many times the entire dataset is passed through during training\n",
    "num_epochs = 100\n",
    "\n",
    "# Image size represents the dimensions to which input images will be resized\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "\n",
    "# Patch size determines the size of the patches extracted from input images\n",
    "patch_size = 6   # Size of the patches to be extracted from the input images\n",
    "\n",
    "# Number of patches is calculated based on image size and patch size, representing the spatial information\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "# Projection dimension specifies the dimensionality of the feature vectors used within the transformer\n",
    "projection_dim = 64\n",
    "\n",
    "# Number of attention heads controls the multi-head self-attention mechanism\n",
    "num_heads = 4\n",
    "\n",
    "# Transformer units define the size of the transformer layers in the model\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "\n",
    "# Transformer layers specify how many transformer blocks are stacked in the architecture\n",
    "transformer_layers = 8\n",
    "\n",
    "# MLP head units define the size of the dense layers in the final classification part of the model\n",
    "mlp_head_units = [2048, 1024]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c946bd9",
   "metadata": {},
   "source": [
    "## Creating Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a9bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data augmentation\n",
    "\n",
    "# Create a data augmentation pipeline using a Keras Sequential model\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        # Normalize the input data, typically by subtracting the mean and dividing by the standard deviation\n",
    "        layers.Normalization(),\n",
    "\n",
    "        # Resize input images to a consistent size (image_size x image_size)\n",
    "        layers.Resizing(image_size, image_size),\n",
    "\n",
    "        # Randomly flip images horizontally (e.g., for left-right transformations)\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "\n",
    "        # Randomly rotate images by a small factor (factor=0.02) for additional variability\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "\n",
    "        # Apply random zooming to images (both height and width) for further diversity\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",  # Name for the data augmentation pipeline\n",
    ")\n",
    "\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0de207",
   "metadata": {},
   "source": [
    "## Implementing multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62c65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP (Multilayer Perceptron) architecture as a function\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (tf.Tensor): Input tensor.\n",
    "        hidden_units (list): List of integers specifying the number of units in each hidden layer.\n",
    "        dropout_rate (float): Dropout rate to apply after each hidden layer.\n",
    "    \n",
    "    Returns:\n",
    "        x (tf.Tensor): Output tensor after passing through the MLP layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through the hidden_units list to create hidden layers\n",
    "    for units in hidden_units:\n",
    "        # Add a dense (fully connected) layer with the specified number of units and the gelu activation function\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        \n",
    "        # Apply dropout regularization to reduce overfitting\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Return the output tensor after passing through all hidden layers\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17cbd8d",
   "metadata": {},
   "source": [
    "## Explanation of ðŸ‘†\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate): This function is defined to create an MLP (Multilayer Perceptron) architecture that takes an input tensor x, a list of hidden_units specifying the number of units in each hidden layer, and a dropout_rate to apply after each hidden layer.\n",
    "\n",
    "for units in hidden_units:: This loop iterates through the list of hidden_units to create multiple hidden layers in the MLP.\n",
    "\n",
    "x = layers.Dense(units, activation=tf.nn.gelu)(x): Inside the loop, a dense (fully connected) layer is added with the specified number of units in the hidden layer. The GELU activation function (Gaussian Error Linear Unit) is used as the activation function for this layer.\n",
    "\n",
    "x = layers.Dropout(dropout_rate)(x): After each hidden layer, dropout regularization is applied with the specified dropout_rate. Dropout helps prevent overfitting by randomly dropping a fraction of the units during training.\n",
    "Finally, the function returns the output tensor x after passing it through all the hidden layers.\n",
    "\n",
    "This function allows you to easily define and customize an MLP architecture with varying numbers of hidden layers and units, as well as dropout regularization, for different machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a4584",
   "metadata": {},
   "source": [
    "## Implementing patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22628bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom layer called \"Patches\" for extracting patches from input images\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch_size (int): The size of the square patches to be extracted from input images.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (tf.Tensor): Input images from which patches will be extracted.\n",
    "        \n",
    "        Returns:\n",
    "            patches (tf.Tensor): Extracted patches reshaped into a 3D tensor.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        \n",
    "        # Use tf.image.extract_patches to extract patches from the input images\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        \n",
    "        # Determine the dimensions of the extracted patches\n",
    "        patch_dims = patches.shape[-1]\n",
    "        \n",
    "        # Reshape the patches into a 3D tensor, where the first dimension is the batch size\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        \n",
    "        # Return the extracted patches as a 3D tensor\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33329ec",
   "metadata": {},
   "source": [
    "## Display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c4e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFICAYAAADd1gwNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9jUlEQVR4nO2deZRuZ1Xm3zN8Y1XdunXvzb0JMzigYtPLtlEbpZ1QFBRBQ0JIZA4xErFtIItliwq2Gm0aWiAgQwQSAQkYRBQVMaCrsZeCi+7lhBMBMhByh5qrvu87U/8RUmfv5/3Ofs+pqoiQ5/fX2XWG90zfe89+7h6iqqoqRwghZC7xF/sECCHk3zKcJAkhxICTJCGEGHCSJIQQA06ShBBiwEmSEEIMOEkSQogBJ0lCCDHgJEkIIQZp2w1/8fXXKrtLok4UNa/7b1depexffsPrlB3HiTiOPpBl45gvfNZzlf3Kt15nHKvLOHrdCy57ukNe/ZvXN+8P2+LxvA0Ez7/kMmW/7rfe3ryxh32NkisvvkTZr3/XOzuMA6Ma9+6Kp1ys7De8+10djtv+HJ53oR7nTe9pHgff86osG9fjtlfC83HOuWvfcYPe38n99bHLstnGbf/rM5+n7Ff8xhvgPOvtC+O4d28rzgnW/cyP/xdlv/y1r2w8lnX+oW2vufpnlH31NS/X5+ia73tZ4jNrnqte9dKXN667B35JEkKIASdJQggx4CRJCCEGrTVJT/JRIhD4/GCqfQPika//tVvnbWuJeXhOsL2vC7bXPueOZV2Tpw16O7cey9IVg+fk1EmZxDH+2yr1IRzHGrfbu2Ad11tvr249jgdeu7zgFjo93jupFZbwzeLdZrlf2bzu7n31NVVlfbAq8GkkdcgqcG/8d8E6MGiF4thxx3H0fYNt4VCVsPdT8oxfkoQQYsBJkhBCDFq72/j5rkBv23CxQp5NAuPIz+xgCJBrPw5+vkfGp7/lfrdx1VK8d8b98N1vsa31DJxzaQLXZLjQ/nm3v3e9NIG/WO62JSfYA/njNO/qH6n+SxUYJ0mbfwaee4Yuo/K2w85c2uvD/jKsp9DrzBAge6xe2tP7inGSYGhOc1iSP46+d1ZYTxGBnFAWjdsiaaLfhUpoBqXnxkNIUySuxxxlPvySJIQQA06ShBBiwEmSEEIMWmuSqBVaWLpUSMNLYgyDMMIEDqBJ4vUoTdLQK/1jh+9LkjT/W9Ql1TIUboG6jR1C0z4tEenB9XTRebq8C6YmGUDrkAfQJEM6Y0dN0tPwlCYJ9xXTBYWG11krrJq1wqoATTI+HE0S0x+jSGuuZVE/lzIKaJLGOLGnSer7GDp2CH5JEkKIASdJQggx4CRJCCEGrTXJfk/HXVVKjMGt9R+U9hTSoXr6lOKofZykXmkOY+p3vvQJGqXQM0Ppj/PG0uGL+y/LhqD2acZJGuddhVTG9tXc7PTHALhlJ+2zw9bWtqGzlTF4ba4MH2Ei/uBpknBepREbGBonEsHLMZxpif8PIGMdO6aOyt9KHOF/TsA3WYdPtNj7DdbLXmm0GOcfubL9mHtjd9+FEELuO3CSJIQQg9buNrrBEj/0odndRtfVGwf+qz9S7raDdV1qvWg817SDu22lSs4dC0JZVHBKh6pIoW09t36fhCJZOrnXB3hG3q7Gie0n3WxvnAPsrfZtcal2JB24wfhbke5r6L1BGcc174u/yFKGnQW+o9ANdjLcpoN7Hbp1GBqoootirKwOO1eJWNf9WfNLkhBCDDhJEkKIASdJQggxiKr9OOmEEHIfgV+ShBBiwEmSEEIMOEkSQohB6zjJX33Da5pXBuIkdWsAHRH14it+QtmveONr9Z5GSmOXlLefes6PKftV1/06HGvu6c49rnVOL3jG5d7Yr3nbm+CARgkz4y+42/N/9DnKvvaG6xqPG8wlrGR5LP38rnr6c5X9mre9uXkU7wLa5zBeBdfz2uvxetrL57KsHra9uPLSZyv719/xFn2KVgyi1xi0+b7hOM4597ob9L2T5cSKAtLrjLYKJZQwe/EVVyn7V17/an0s4/l6v16jBcNLf/JFyn75q/6HPkejJJtlY2rhNT/9s8q++r//fOM4Xkk5o8UGXu2rXvZLLgS/JAkhxICTJCGEGHCSJIQQg9aa5ObGeuM6L9TSDL20szTX19aa9wy2VZAlzGw21mGcDinGXbORz50713GPprFsTe7MmdP6D3LzkCTZfhh3FsaxFK4updGQM6fvar8xDCPb62KrXWRz9aw+lMzNh239V725XcE81s6dUfZslu8tZ1mu1pWe/tl8HsjZM/qa9G8UNbtmG7VC5NzZ5nFQN7Va14Zayq7Cb0jpmdhiwntI5qGD8EuSEEIMOEkSQohBe3d7c6N5ZagyuVEhG9ne2tT7ylJpRodDXB9y83Z3dvQfrOZ6WGDZPPK8sbY77rE/vGsSoEvldcxTtn2FW5v6GcXCncWSVsG2lQbT6QT+0r4afinehTKxz2G6u6VsryunHAXGLUQHwyzLzHGcc25jVbvb02ntYk9nev841d0A0t5gbzlUcjCbzeAvMgQI12Dokdg24AbjNWtXvX0IUEiqmMH1lOK+l9Dt0ausT3ebEELuPThJEkKIASdJQggxaK1Jbm9uhTe6B5B0pMZj6T3OOTeZ7OpDCY3L75i2/86C05nWu3T4kB0v0zWspUCtqkv3tg5DFbkOIZHXhOtQ4ymKWuMJ3jt4RoNBrZWlg74+B/NINtjqQGpplRf20ZyaVgXCWEp4F6ytsUthLu7r7u4ubu6xuaZDWSZCh5xM9TMaLSwpe0FolGlq/3QT7AYqlj192ktTFCl/wQ6dehypYeKe/pGkThpqR4F28+/VCmkKxk7NgV+ShBBiwEmSEEIMOEkSQohBa00SY4+UCuCVLNMovQ/j6IAY1uv2rbYmqcpjBeLIPM3G0CS9FrMq7jOsuqF+1C10sH2Mab+n4+rkNWWgxcxAw1O6aUC3ySZaw+uL60vx+cF9jtUzsy+o39fXI/Uu1HmLvHCwcb1YwTqgyqbaNsqfoX6XSU1xO6zb72zq9N68FPGcVfP77Jxzg36t9/YHQ3Oc0QjWK1kuoEmqMmT2uzAc6nGsMmsYC1kKHTwv7Gc0FLq3c84VIjaywDhJT5+W74I5zFz4JUkIIQacJAkhxKC1u338vPMa14W8x1iF8dhbHzt+HPat3TXfvW6uChQKYzl+TI/jrH29akNiuUWQy7ETeiztcIb2b+/aHz9+onGcya5OWYxBPtkRrlA21e4nUs30+qQa7y0PQVrog9vXE+5Z6BktLx9Rdp7VYUvTHe3aTqGqd1SIcJpCh9YgVa7DoSKjincBbmE+rd3t2QTTKH1mEz3WcLEO8zmyoK93eUW/N8vH6ufb62v3E1lZOaZsszK5VQUo4J+urKwo26rs47nf4l6iy4wchd+rdNUxLRGrAh20ISy/JAkhxICTJCGEGHCSJIQQg6g6qMNOCCFfxvBLkhBCDDhJEkKIASdJQggxaB0nec0bfk3ZVoSbmZYIsY0vuvwqZb/yutcp24qxxBRGK07yxy97jrJf//bfgHNuTv+z4iZx3fMufrpD3vyu6/V5qzJPiBWjqdc966KnKfutN74djlXLzTvQFmP1rO54uL66ure8C+0ZXvOaNyv7ec+9TNnHTtTxeysndKzmAsQ6jo/UcYExdDF85oXPUPZ177pO2bNJHeu5tbaq1u1srCm7EmmLVaZjE1/2y29Q9s+95HJlR+K+YfxeBiXndqf1sTc2dSzqW274XYdc/JTvU/bx8y+ol0/dT61bhthAGTeZ9nRJumddeImyr7vxHcruEidZqjJz+vqvvOyZyr72+rc0HstPQzTaN0D86dVXPl/Z11z7GmUXKk5S7xu6PsnPvfDFjevugV+ShBBiwEmSEEIMOEkSQohBa01yOBo3rgtmH3fIqR4MR8qWpbVC7RtiqwUDjgOlpszz8nK3o6ZVDWPpPNvI0CT986ivuQr8m9bD8lmi7SaWa+tBCbOeuJd54KLSUmtAUh/c2dbtc1dO6pz/Y+LQw5Gdf1xATnUp7HyqWyXMIJfbSZ0qUIarwParIvcXNUhs+zrLZNuL8DfHcLSg7KWlo3vLK8e0njte0nquLI+GJeiQtKefdydNUraUxTYZOA68V4UsrRbpfSO0xbahe5emWAaw3r6Afbtokm3glyQhhBhwkiSEEIP27ja4wZY3663q4Ab3R+BuG50WvSrmXdxtvB6B720apdJa+Nuea6+Wm11557SLXQVcEq98Vlm7ij2oWp567nZ9bLuwmHMJhHZsrq7tLW9BubAc3LX+Yi3bRJHuBogUUDFcu9s63Ga6q93tuJTLtruVQ+k35W6Dez2d6G1nokRbnDa/U/cwAnd7cWl5b3llRbvbWGYuls8wJInA8+7mbjdXF0cS7NpYSBca3W0oZyfW428ZwetR7nnQ3WZlckIIudfgJEkIIQacJAkhxKC1JinbKHig1oCrlYgX6GJo6IzBECAZWhPQbPBYroPO2DUEyEyf9I+uLK1D2vcuSvD+1I8XUwAd3LvSiRSxMhAyk2udTmp6010dmrOxqtMH09tr3XR3W4e4IOvn9L6x6HqYTSFsBzvmlTI9zhai7E6Lel98ln3xu+gvaL1xHsvLy8peGNf79KA7JOp9qtNo6P2Om3U6S4Oct2+ncZx1r1HfFJ0iy/a/Oef0bz+u8Fox/VF2XO0uSvJLkhBCDDhJEkKIASdJQggxaK9JthHfGpAxTaHjWLGQuM7TKTpokt6+ps54sLRES5NEKlxX1XZITYmsuFGIi6xgmKJqr0liql6e1XYx0+s2VteVPRMpgli+DVk9fVbZ40H9uuaoSZbNqXUuEOtX4vUqTUsfN4H72O/VsYwLS3bcp3N+C9bxQh03iil+qM87YYfeBXznrHhHa9uQPumvr+YuOjcvRlH+oUNrZafnFF9nhN9BzDhJQgi51+AkSQghBh1CgJrn0+CHsqrkY2+dQKhRJ3e7Q1oiuk1RozHnWCosyRzm7m3gmnSxcQz58QKomk8MSKDySyI2x7QutGVVmTzgbk+nkJonqoAX4IpnUBVoR+ybwXGQtTPa3S4Wate2gpTFygvjqeYvz8GqEoPVadJU271+XSF8NAqnJS5AmNBwWF9TD55fhCl/B3C35W8DXW+8fmtbbxwMLXMy1AjWeOmC7b/RrN96WFpoL8PNg1+ShBBiwEmSEEIMOEkSQohBVB20bC8hhHwZwy9JQggx4CRJCCEGnCQJIcSgdZzk9e97T/PKQBqfSo+DwMJLn/BkZb/zA++DfUX6kdEd0TmInYJ1T37s9yv7dz70h3jSzRjXh+M88bu+19v9/R/5YON5+nGSUPZJxqzBv2lP+rZv1+P8+Z8pW4b0ba/rsmNn7rxd2advv3Vv+c5Pf1atu/aNb1f2U75fjzuZzMQytDsooNugSEtcWNBxhTf/2V8q++Iffpyyl5dECh+kovWw8p3xzr3qTTcp+4WX/zDsK1oQYIxrrH8yfdGO4ejxU2rdi376lxzypjf+mrIXjx6rl1eO640xlVTEA5bwXwlP+k793r335j9SttWSoUv7hqc94UnK/s33v1fvK9o3FFC+zrfrdwHHufLSS5X92uuvbzzHArphYqk0q0zci553hQvBL0lCCDHgJEkIIQacJAkhxKC1JollnCzMPMtQW1QsWS/LrB1AkwyN0yVYVGVTt0je7iVYhl+UP8P7ETdrlFVktNBwzvWh/H8qDpUPdLtZzDMeiBziUEL6DHTGmcjXzqC1w2ymS5pNRIvWKJAXvLW+oexYtMgd9UGv62lbtj5IU/u++bm+ok0sCNKYqixrAPRFHncTo6FuEyvve38A+4MeWop3ugjcO/y9Wjoj2roUYLc2Ea7aX+52KFwbf8+VeY6Yt96sSbaBX5KEEGLASZIQQgxa+9B9KK2lyg/hxl518faf7+iyxMrdhorDXrViuRxwt3t6HPiY1xsbx2pTesm/JuFue9WnsSxb7XJ5rjkwhGtKhJtRgru9MBpre1zbQbcRzlGWVtudTtS6DEuYKbfPLsk2g1JqE1H7rcq1K1qA+z1Qrqt9PSgJFEJOSEAq6Q/1tQ/F9WD5uXmMxvq+j4XsMRpqCaSEd0verZC73YNzscJtMIQmXPywBssAyvCbCEuwlc1ucPB31KFUmu9SHyzzml+ShBBiwEmSEEIMOEkSQohBa01yAHpXF51O6we29jBA/U5oHl7ID6Y/tjw/55wboH5kbG61ggjIhF8YC68pkoZ9QLk+EALkjeNElzhYl0MI0FholF4oChBDSI3stDiZQVsFkM7ku1AFdLUMw4diEcpRohamX2V5j0Phazm0nJiJa0hTrdfFabPu6Ov2PmPQJIfiOQwhPKiIUPut71fu6Yga1CSlbocapK8H2sduOq5zWg8sDQ0Sxw21W7G7mzb/38Q9Rz8I/JIkhBADTpKEEGLASZIQQgxaa5Kol5hgnGQHTXII8Xyx0OFQe8AZXmscNjiOs/Y19M02LWVH3jVZmmRznCSmqSEDiOmrijoFMINQsRjsVJxHH+8NMFzQulqytbO37EXc4TMTaXxWm2LnnKus6FWMmQWdNBUpmoPA9eB6eeg01frseAxartAYx3Bf5jEe65ayMi2xB2ml+IxkjGkUuHeow3Zppaq78QbSBeEHIHXIGDTIEtvcliL9MwmN03y9VkvceXZX+CVJCCEGnCQJIcSgvbsdcFkUhrsd+vQd9LVbn4jPbO8zGodVp9DNrXdWSIHhvLdzt/U16X3QvcYQoPbudj/R7poME0FvJoFxU3HskHs6Qne7V79GBVwO3p+ecLejxL4evDUyYMirUo/utgiBCckHgyG42yL9EdNXsXqScrfBlZ7HAty7VLjbKYZwoasr3e1ACFAXdxvX6Ure9ndUBO9r5JpTDb1KPrIaVuB7DeUFmfKIrrinWMnuBvvwvPklSQghBpwkCSHEgJMkIYQYRNV+SvUSQsh9BH5JEkKIASdJQggx4CRJCCEGreMk/+JvPmGsbV/mCGOl/uPXPVLZf/XJv1G2at/QJS0Rtn3EVz5c2X/3z//YuK8Xuuj/QZyDXvfVD/0Kh/zzp/+58Xh+ChWUnpI2rHvYAx6g7L/5h08qe3ur7ja4sXpWrVs/d1rZp++6c2/5M7d9Rq275pprlX3pZU9U9uc+9/m95Ts+d5dal8A5D0Sa36CvX7//87G/VvZ3fus3KHs4rGMfFxZ07Oki2MtHlvaWjy4dUete9oq3KPvnX/gMZVciBhFbMgyg7cXyyvG95Qc+9KvUusc/6Ucd8om/+t/KTsTx0qGOs8zhvwtm4rxyKDP3LV/z75T9Ufi9yvJoWGatgBYbueh4mWW6jNwTv/27lX3Tn3xQ7yu2x5JsRYFtI8rGbZ994YXKfvONNypbtv6wuj06B/MGrLv84qe6EPySJIQQA06ShBBiwEmSEEIMOrSUDbQZNejU6hVycCNDk0S9C9eb4/Tw0qVOiFsbrWtbjJUmmEdbL/ttcVGTFPmtgV4RJbQh2N3e3lveWF9X686e1RrlmTO1vbG5ZY6zC61epVZW4Q2BHOsoEc8zkLsdgx6YitYeKeRj96CU32Ag2yLofGtkcXFRjyuWEy8nXP8OhqLVRZqEvzkwlz1ulst8DlDxC4qJBQ4scp1DOdXe+9vcksEaNVQDwVsv87G9aoNWqwd7nLljd9+FEELuO3CSJIQQg9budpo2z6f+F6zZetAk8coeyfAhvS6xPqsDA1kV0q2QH/xDm39lcCztZtjnWYhQh6KYGVs6t7Ozqey11XN7y2fOnFHrTp8Gd/tsve3qmnbNkc2tbWVPp/V5lRC24lX7kt30zFGcK+HGF6JsV1HCOrDlvljh3APfK/G8vPfR4QUJqaHUcsc8SlEt3jnn4lKGvmCGcHPGcCiX2L/mSK6EjaNGO9jF0HCp/ZJlaFeN67xxYL1UNqqqeR7wbLrbhBByuHCSJIQQA06ShBBi0FqTRP3PKTe/vYYXkgQSQyvE7mtWO4coslUbT1tS4Do8p8pYFz6e1khgLBDx8tlkb3l3okNvkPW1VWWfPlOnHt51+i5YpzVKpUmu2prk+poOEdrdrc+rzCH1LAVbpJDlha1KZnCsRKTPpZlOY0unWg+cCHua2Vohpt5JibICrTMu9HdFJp5PPp24ELhN3BehTKXXa1JZXWoaerKj6oCIxzWDc8xxfE2y+beOvzn5G+0eAiQ1Vnv+6fJ/FXPH7rwHIYTch+AkSQghBpwkCSHEoLUm6cc4WeW+9p8/hXFp6rihczLWecfqdIqGGtTmON6Jy5adMBKUfdrd3dlbXl+3tcLVczr28ezZWnc8e+6c3nZV65dr4tihtMSd7R1lz7I69q8s9QWVoDvmUa27xbGtFcrjOofvHMYv6ps86tfa32ig0xuR3YnWCSsRhJckoElivJ5o3TrZ1fdlHtNdHWMai3bDKcRZVhGkbSpd0VYocb0yg+GYXTS8Dpqkl5IpU27t67H+ryJ4TgeYj5zjlyQhhJhwkiSEEIPW7rblcuInOabadfncTZLmtERv3AP0eTzIJ7h0XfbjbatwDAj7yGY69XBtvXaL7/zcnc7irrs+r2yZlri5saHWbe9AaqFwObOZHWqUQ7Uh6VKjXFCg2yfWl4E0vp0d7b5mwv3Ocu2K55m+b6m46UkgeAblhVxUTE89dxvSH4Xbv7VlyxTOObe5qVNHI1GtqLewpNaVCbrM9Vh4nz1KvO9S4jm8Bqnez0hVNbLvndQuPGkBSKBi1L9mj1d+SRJCiAEnSUIIMeAkSQghBlF1mAIFIYR8mcEvSUIIMeAkSQghBpwkCSHEoHWc5N/f8vfKlrGRGNuYxBDzZKQwPvR+D1X2Z+78DOxqRCKW7eXUB13wYGV/9nOfadjSx0/xkmXn9fk9+H4P8fa/FccSMW5ZpmMSJ7u7yv7Upz61t3zLLbeodT9x1YuU/dKfvVrZp0/XaYlrUEZtfV3HTW5u1jbG+/3fT+hn/9UPf5Cyi0LGPuK9KsEW5bEgBfWWT92h7K/8qgcqW3ad7PV1quEA7JWlhb3lo0cW1Lp3v+/Dyn72U79P2QujunxZP8X0R81oPN5bPv+C+6t1z3/Jrzrk3W/7NWUfO7/e59gF91PrykR3ZsxFLCFUkXPf+vXfoOw//cTHlS2fUVFA6iimkspYVkgrfcJ//jZl/+7N+l4WuUxRhTTLqgBbxNfCT/miJ/yQsm/8/ffBvkZriw7/zfLUH3xycBt+SRJCiAEnSUIIMeAkSQghBq01SWznqsUZu8y8XBss8YQ5qUK3OljBo/1jlYLbT+52JnKfMZd3DUqanTl9eu7yPKSu6JxzudCH8L5H0NBVlta32u0652vOngYtKCA3XWtcgRYb3mnIHHF93KLQG+9Oam0XrxVZg9ztoqifTy/V14a3Jhd63jhQYs45/3mPl+v89Bxy5qM+6KEijzzClgWA1ypB2H7rkvatbOcMpM1Y/jbgd1Pq61HV20LtG0C/1q9z8/8ZzLO7wi9JQggx4CRJCCEGHSqTGy5noOyYFTJjbeucU+Ey6H/tp/PZfjArr+/jU16GSWAJs9PY1VDYuA7ZAleuKIRLWqHLiZ3r6mWvpBUQeyFftY2uegHdBWWZtTJQ7sv3+utzLsHdznN9PTu79fpQ6bf1DX3f5LH7qf6JeO52Id1tfZx5oCRyRJSsy6f6PJMYfp7CDn3d+C61IXNYr3Cwwn/zb8OT6MDUP53AOxdjqTRZ+i1QNq5hv7bwS5IQQgw4SRJCiAEnSUIIMWgfAmR1MTxgNzI42AF2bb/vQbaVuluRF7i5x3Squ/FtbdXalWyx4JxzZ0B33NyouxjO4DgIam9SfykLSBEDTU9qlqHGdClokv1enRLYh/TAAtLapE5aFHb7hkFfp+VJDdN7fKA1Se0zMIzb3tGpoPK+9UCThEt3hQgBWoKU0nlMJnob+Uy9EKDeQNlJUt877BaJ+CFA85fnbQvBOeY4VgiQt7KCkCbZxiSoSULbDPVaQQiiEQK0n2ggfkkSQogBJ0lCCDHgJEkIIQaHokkiXgrcQdq3Guv+tdIU/Viw+l6EYv2c83WoDaEznjt7Vq1DTXJrs942FO9XQJtVS5Msi2ZNMvS4UJMcDGodcjQaqXX4LpQirlC2iJ3HaKg1Obk9xkkWBdpl4zpke1drvZnYt5dgCiZoY+LydnbCmqRs3eucc5mIjcyhnXDah/vTE5pkQDiO4b7HMsa0S1piME6y+Q8RzhmeIChaygbEQj8tsRDL9jCqhTPjJAkh5HDhJEkIIQb7Tku0PlsPksJopgDitlhlRGwb/Kj2Uhz3d04y1KQJrPQtq4SvQ8XwjbU1Ze8KFw5dZsRzqctmd9tPU6wJhQAlcD9k6t4QwnZ8l6tenIF7iQwH2t2WflOW6fPP0a0vpbttSyKzTN8buXkG0gJKDT0R/jQLyAfOOZfNMrDre4BpicVAb5sq2SDwfWP6oCiHNYcLhfB+n/LlwdI+lld/iPMCokOA6G4TQsihwkmSEEIMOEkSQohBVB20bC8hhHwZwy9JQggx4CRJCCEGnCQJIcSgdZzkLXfc0vqgZkl3iJt74MkHKvvWu25VttX6Ie4Q0XX/kw9Q9u133aY3iOYufmHcuNFePafTCr/2qx7hjf2hmz+g7Ns++5n6PMSyc87dcbu+fpnGNoE4ut/8rd9T9iUXfp+yCxErOJvqmMRdSJWcimNjuuBH/+Jvlf2fHvW1yl5aWpq77JxzKZYaE2l+U7ie337fnyj7cd/9zcre3q5bHUwgvW8X7FwEO2ZQzu5Tt3xe2Q972AXKls/Xi4uE7onLSwt7yw+5//lq3dtv0tfjnHM//YLLlP2gBz9k7rJzzi0sH1f2cGmlPseejkf9pm98tLL//OMfVXYhfkeyw+Pd61yjjese/+jHKPsDH/1TvYE8NsbiGv/9gf818oTveKyyf+/Df6zPUZXcw1YeOdgyhVGPc+mTfqTxnO6BX5KEEGLASZIQQgw4SRJCiEFrTdIilFcZq5aj9rzsl0QSmiSOe4DcbS+nWB3XRrawDOUfO+fcJrQaXV9f21uWrRycm1NKS+iDVaAsG7bWlNvjOrxGmXIbainr71v/JYlRw9OvmNIoAyG6si2Ec85lYl/MqbbOOfQuFKDRVaL9aok7wzhS+2yTx5+D3ptnIncb3iWv9J3U3hK7/FtkvCt4q4xqZy4K3Dy/ZJvUJDF3G+32IdoHyd0+KPySJIQQA06ShBBicK90SzzMUmlqHbrX1nHMUeaMY3z6Y/VxGXKA7vE8tqFUmnSxMQwGwxmkm4wus4d3CfUf0CvC7nOxCHsJeTJedz1xXlgxHO1KdMwLPSOrLFk/h66MfRi3ql3VLA/JFBAS4/nYcp0+p0K52+HOmdK9vtuuz7PI9boyxw6XtR2VgZ+u0R3Ad6+bf1ehryi/VJp4vngfwXcPvc7mOPI493JmNb8kCSHEgJMkIYQYcJIkhBCDe6Vb4kH+u97cNxSO0HoUf1tV3B7EkhxCMWRYzu7uTnCs7W2tSW5v1el1M+iA6Gt6UpMMaS/QIc9IB8VQnUTof3HeLbxCd2XU9w71XB3GZF9PAp0Ke736dUVNEsNvZlKHjILKmj5HcV7+LW++N/js5oHnKcN8igxDfqAdhNSrg106m1sy+Gm2oE8bUTyIGS4Gtz2Cg1VioDZdR/XO7TdVUwjbNxBCyOHCSZIQQgw4SRJCiEH7tERLe/BWYd5TNH+51bHkKtArMQxLWbb2EIE2MRPxilPQCXd2tsGudcizZ0+b4zjn3CbESU5lG1FMiYsSsOvlMnRNidbpUhELGYO+h89BHjnUgjVO9GsjLyEDzc2Mew3IQ1gKL42bYywxZVMOm0J5M2Qw0PdtIL4dBn19reOhbnN7dHG0t7y4MHIhBn18RsKosO2vtuOo1iSTKPCM4F0pxR3z441Ro5THsfH+r0K8DN6jh9+c0gcDWmEXlRx1x0qVb+twoC/AL0lCCDHgJEkIIQYd3G20OyQFRs2rQvvq0AU7LVG5b6HPanBzM+FuY8iOrNpzt726t3z27JnAQHMq/czq0I4cSj+X4G5LF7sK/JuGbrAMmUkwjQ3uZSkrVwfS+BIYR3o3WYaVcJofOIYhIfi8k7i+N95R4XnK0JQUpQZg0NdVvqV7vgDu9RK41EeXxmLd0BzHOd+1l5mXValDfiJwtxPX3t2OQMuITMkL3O2G5XnEGE6k5INQWqIIbwtWnjLW4ziGW88QIEIIOWQ4SRJCiAEnSUIIMYiqe7vOECGEfAnDL0lCCDHgJEkIIQacJAkhxKB1nORt5+7Y9yAyZg1j3y44er6y71y7U9mRkU4VYx0nEStXFjrG7OTJ+yn7tts/rezTp++ql8/oVMPV1bPaXju3t7y2uqrWveTqlznk6pf8pLK3RPdE7LaIdi5iKjMo/f/u9/y+si+95IeUPVTxf1p6xvJvctydnV217n2//xFlP+67vsW1BTseyrQ8GcfpnHPvvOmDyn7GRY9XdiFSHldXz6l1a+vryp6Jd2EGsaj/76//Rdnf/KhHKHs8rmMhj0Bc5PLiWNkybnJxpOMtf+V1NzrkF1/0o8o+dfLk3vLJUyfVuqPHTij7yMp5e8tRX5/Hv/+mxyr7Y3/5YWVn4qdeRPq+F5ERRwq/1+/4xkcp+yMf/0u9vUo1LJvXOV1aDltffO9jvkPZH/jwh5QtyxXi7wJTY+WxMX316U95qgvBL0lCCDHgJEkIIQacJAkhxKB9+warpaP3h4OEXrZvKeudkzDzQDn4DLS/jY1a0/r8nZ9T6zA/+5zQKLe3dRm1eWCLh6Jsbi0Qx/BI4nBLgL1DYW67SAxOMA8a2rXKPOpQq8/hUOt0siVBHsjdlq0Deqn9+g0hb7oq6u23t7X+h60eUpEnHCoxtzDSOdcrR4/Uy0cW1brlxQVlL47qc+yn4YJew4Eeazis7dFQr+v3IKdcPLMKewQDXq6zcQv8SoeyJJ09jjkvYLuGClt5iJzqwO8V15eilYXXMsSzxbbM3SaEkMOFkyQhhBjs292WH61YYRojc6QbGPp89z1ouS+u03N8WdWf1X7JLs3urg5zWRchJF4I0DkdArS+sba3PJtBR7s5ZNAFzyr/hm6j2Y0O8CsyC7cD3VEcRywXA9v1QTc4m4lOi3C+6FJLlzFUwgz3lS5mD6qNoy0ricX2MF618KHsygjn0Idx+r3aHvbCP6fxeAh2HcqzMNauPLrmiag8X3bsAOmMUDq/MrmoaB8YJ/HWCxcaVnnzQod6471UPyO5J1ZH92xxjmVIS5oDvyQJIcSAkyQhhBhwkiSEEIPWmqSnW6i2Argt2h00SU8vMbQU1EmFJjeDjocIhu6sb2zsLZ87p1PeNsQ653S3xFDowrxtYiGSQSSOi0C4kZEeUSCUpSp1uFAhtNAE9o2hs2Ii9LQycE3DgdYkZRdD7EyYgj6kNcmA3hXj823WM1ErTMS70gu8c304D9lJs8QUNwgdK/r1uOlI35d5jEc6nXBR6JCLCzrcqA+hVlKTzFxAaPVCgJpTg1FHljqj1w0RSEHwjZxoyYDfYKiZJ7Xdw/YiwGik70VZ1OFRRaHfe/w/AJlyG3q358EvSUIIMeAkSQghBpwkCSHEYP9pibIiEuhdlakrBsbBYCp5nEBLWZn2NAXtCNkCTVKWL0MNEvXL6Wyyt+zHiflgOlYsSlPhvcJA0VjFrAUGAr2lEOXiygTvnX70Mi0xlC7Yh7hCqS32Cr0vnrI8DUyV9PBag4qWqnAzsCSbbFEaalfaTyGuTrzPkVfuC59lfezBIKxJLi5q3VHGSQ6HWq9M+3A8EStYBn66vVSnNEo918UQcwgtgqUOGWr7O/KuWdw73NjIlAx1kVmC+6ba0cK+niYpbGqShBByyHCSJIQQgw4hQF3WVo1rQx6jtT7sbdaf0rOpHQIkw3ic02mK04neFz/f5ThxIIxlLjKsp2pcdffxxR9C4RieIlLKisx6X5QA5L+XIbcew28iK70Q3BvpMnuuLJBBGJcMx4nhHRuCBCDvZMjdHmCKo0x/hOerq707tyBCU44sLpnjOOfc8pFlfTwREhSj2wthWnGvdm3jWJ8HsjDW7ulQuNgRVJpCd1u62FgtCjl65Aj8RbjbXo4xhvDN22s+x1eONY6DskxRoOwkfgesAkQIIYcLJ0lCCDHgJEkIIQZRtR8nnRBC7iPwS5IQQgw4SRJCiAEnSUIIMWgdJ3luXbc0kFJmSNWU8VAYO3Vs+Tw9zpoexyo7j3GDOzt1+uCtt35WrfumRz1a2e9+zzuV/U//9A97y//4j/+g1s0yiNcTsX6YwnfD237bIVdccZmyezKFroTUO4jxmk5F/OZ0otbd8M73K/uyix6vBxbniWl7oxF25qvXwym4V79R36ufuPwSZSey9BvETFalLjVWqs6KOnX0tW97r7J/7Gn6enKZXgblsbAcnWoZAu/cdTfdrOwrLvoeZafimS4u6JYKK0d1nOPx4yt7yxecOqXW/cCzXuyQj33grcoejevYyjGUSusNtZ0O63OJUl067PyHPlzZd9z6aT1wUsdVRhh/CeXOZGwkpn/i9a+JtifOwX3HgFsvTrI5fnUJWllsQcdRdRywvTYmxgS1BPd8HvySJIQQA06ShBBiwEmSEEIMOuRu7z/vUksPdh6t30JVCZrmOckS9WVpnxWWfJc2llNCTaNLLrpzc0rJyeOjfmLkvYdyqqMIx5G6MVwTXGOp8rztcbBNhDwxbPPhtZQd1NqYLME/jyXQA+U5YtsAbBshx00Dpd++4iEP1vsKfXY81trf0pLWsI4s1ZrispfH7NPv6WuW5yZL6Dmn2zXcvW+tI8cDfV7ICMquKU0Sy/uhJinrBQTeuR6cozp0qFZa+46yLu6wcWxooR2GFGMTQghphJMkIYQYHKBUmgyx0Gt8l7E5HMM7qlHp269M3nysUAXiHLrgSXe7CLnbXb/ZMQRB+rOoChi+ri9FaPyCdcLdLjFkBuyivu+hTFXP3RZVr7GEGbqXo6GwA349VqOW22MI02jYbI+GdsXwr3iodrflOQ/huOh+y/W9HpZr8+lDqbVUlCnDdz+BEma9fj1WGnK3cb2qVI7uaHMnVHyeSB+lDCH5eHsalclDmp317uP7iqGB0g7NP3PH7rwHIYTch+AkSQghBpwkCSHEoH23RKt1gBfG0px+FNIErO5sqNngsWRKHKbHIRgWEovtMYTAkzc76hqoj5YqfsoID7r7D9IIjITaZ3OJezMEKDgMaLZSz8VnEjdrhb3UfkbHjq0oW6bLjVErRI1SdPHzO/ppTp04oeye0A1RQxwMtJ2IDobBx+OcSyDMJxUhND3Qb3upPu9U6IoYHoTEMf60hebcoRlLFHgZcL20vP+ruJeKMvqhgM62O8IvSUIIMeAkSQghBpwkCSHEoLUmiVqhjnHC0kR6X6kdhtqieuvlvl6pJUxNqy+nl9qaDca0pUqTRO0Tz7nW5DAmdB6YEihLkUWYPol6nyh31kIsbLY9TRLjJEUL1oBmVcE5l1F9zpjuifruUMQsynas8zh+/LiyB/36+aIm6cVJCi1xNLDTH8+DceS7kfZQu8afTH192Hp4HjG8SzJOstfTGiRqlKl4p33NEc4qwt+rkZqHccDKDrxz3jvZXELRb+0r7Y7CobW5d0oHEyX5JUkIIQacJAkhxKCDu90crlF6rpxeLysUh9MSE7DlMlQm98JN6jk/VPnFc7fF9nitBYYEVR1DgDBtSoTfeCmPRvWi0L1DV84qyYLjyjCl4DPyQo3KeYv3/AXOsV7Gyj0IphMORUrjEENz+lgxR7ixHcPBZAUhTA2M0N0W96rNe5GAS52KVMP+cATr9LaxCPuJjN/jF7YInkszHdzTA7myhzQOrgpVH+oIvyQJIcSAkyQhhBhwkiSEEIOoCtXFIoSQ+zD8kiSEEANOkoQQYsBJkhBCDFrHSRbZRNlSyiwgtg9tq1TaEGLDJpNdva/RJgLjAnd2tveWb7v9VrXu6x7xSGX/4R/8nrL//pN/K5b/Tp/TVJ9TUdatH/BfmRuuv8khz3zmhcpO5P3wUsJgZ9XqQQchvumG31H2c572A8ouRYsKjG2UZcec0zGmmJb5uhver+yrnv5Dyo7VvvohHV/R5c7OO1GnAC4fWVLrLnn+S5X9u295hbJl3GS/h7GNOm6wL+IZ+xDb+MjHP13Zn/zQb+ljifjEtGeURnPORSIGE9M5H/Afvssht33iI8oeLR7dWx4vLuuxBrrjYdIXdqzPIxnrtMxsZ6bsUKqpJBLvGb436QKMs61/G3KYg1QsG0CbjMnObsOWPvgOWu0bkhYtN/glSQghBpwkCSHEgJMkIYQYtM/dhpzqUpZEAiFNls66G1GGK1h5CXKM9aBwDrosV5EbrVoBLP0mS1aloGFhKwhVvqyV0mKUkguUdZKabLjMHOS9SxOTqr18e2EH5Csrh95r/Yll4oROmud2abGiwLa/9QXl3ssA5e0S8YxiaIELzGZwHvLGwXvv4uZ3Iw6U53POud5wAexae0tBn49SKPEmnn8wTTzC92h/dA2j1psHWhPv/8CBbQ+3bwS/JAkhxICTJCGEGLR2t32MD/hKz72yVBiWDUOyHCpzC7Ms0L3WbtLuzs7e8mRiu3J50SwJYBm1LEd3W7g9pe3KOee7p9ITQncGq34nKpwh1GkSXEMxrleJ3DjvYEk2cPuly+mXP9PHmmV1aMpuIKxje2tL2YXYtwchTD0od5aL0J1hz65MPp3qcBnlYkOoTdRDmUZ0O4SQnXkMF3TYUzqo3e8IuiNG4NpXXoX8LnRxOas5Swc/6oEw5g1fsUJpSelOnYfmlyQhhBhwkiSEEANOkoQQYtBak/TTmmRXNCNsxzlXCp0tL2xNYJZpPaEU2iF2o8umU2XvCI1rMgGdCSjyZk0Sy/mjXZT1sduke8XNt25Op0lsjSDOMKBJoVYYOxkyok+i8MaRqWg2XtsMoQ+iNojk4hlOAtrnjtCYnQNNsqe1zwGE31SDWnO1WmI459wM3ivZJiHpw/OBZyDDdNJhWJPsj7UmmfTqND8v5Md73u316cCtVXjvXJd9D6JKdgjNsUKR/POHTpFKo+weDMUvSUIIMeAkSQghBpwkCSHEoEOcJLZVFaXSIOYwy3U62Syr7SmkgK0c1TrO1ta2svO81pZQO8J0ssluXc5te2rHSU4xHlNcH7YRxfhDWeqs3IcmKSWTEkO6cGdxn4MxmRAfJktGVfjPIZyU1JVDqWg5PN9ExkaiGHaA9p6GPOulnvn3TR7H63OrwOeTijjZ/kCXBhuNF8GuNcZeC01SapDOORcJ/dPXtw/WCvWLjRcD3EVXPMCxSvxRGeUW28AvSUIIMeAkSQghBh1CgMAWfyjADcwy7Y5NJnWozu5Eh+0gW9va3dauerMbf/c4tbs9CbjbM3C3pduM1aexinccdft8T6yNYFWJd1q62wGXJAK3MhLpdV4wCfiYRSndbds9zSE9tCdCbPwztG6QffP8dy5qXOffNilT2NeDaZgy5Gsw1O7xcKyr+AwXavc76esqPvOIwd2WoVlVh2+Wf6uOeGW8r/eWu+3vi/dRhLftw9/mlyQhhBhwkiSEEANOkoQQYhBVXUsPE0LIfQh+SRJCiAEnSUIIMeAkSQghBq3jJKfQDqEQsXI7Ih3wblvHQk5ESTMsYfbIr/9KZX/sE59U9kykwM0yHZ83hTjJ2aw+9nSi4y2f8oOPVfYN73qvss+evmNv+YxYds65nc1VbW+t7y2Xhb6eN173bof82HMvUrZq3wApnSWW9RIxqJiW+KZ3vF+Pc9kTla1aP1TNJejuHrc+dgHn8OYb/0jZz7noccoeiljC0UjHCi6Mtb0o1o9GOmbwqpe+UtnX/epLlN2T6Y8YEwrSel+0lOhDnOuTX/ALyr75hlcre+XESbF8vlp39Dxtj5eP1eeQ6uvpLR1xSA6/FZmK6KUlGh0PMdovHerWDznEI3f5rwdr2z48z+m2LmfXJU7S2nbx6LKyN1fXWu/rlQwUNsZJjo/o0nXz4JckIYQYcJIkhBADTpKEEGLQWpPMcq2HyXJZu9CSc2dXtwqdSK0wkFM9mWktRbZzmEHOMOZfz2Srh4AG4xUdE+XR0p7Wd2Q5f+egjULVIhfU2CaCMmwxJCGXQnsLteMt4P4oCRM1yRK3rdfngVzn6Uw/b3k/sMXEoA8tCYQ+hG0xkBT2TcS+s4l+xzJ8b+R+5ijObUNr27HQunO85VBGLxYtF6okNJJzVQTtiRuWnZuXn/2lHdLcLf9ag+XP5PbWusOAX5KEEGLASZIQQgxau9tYBVxWH0f3awL2TJQ4w6rlCLr1mXShwZ1Et1DaGMaCoLstu+BF6F7H+jZVwoErW/w7U4C7HXXwBmSkTrDTJHaAlC42uNtYPqxU9852t/EZRXH9buCeQwhNkfvicZAc1heufnewgv3O1pY+J9llMuB+nT6nQ7yiQV0OrbdwVK0br2i3fiDeyTjW7808sGul1TnT5BBrpX2xMpMtlxmxXGo8fy+MTpVK63iSjl+ShBBiwkmSEEIMOEkSQohBa00SQ3NUF0PUKyFdUJb7D2mFGOZihcCgdlaIbYtACwI8D1k639MgI22Xws6rcNhHVqAOVY/t9ceD25MJHTJDzRGYQNqmigFCTdJrRSj0oVCoEdw72QETdcQd6DY4HtXvURoImcHQnFK8R6uQprYGthlbA9x+52llZ64O64mHOm1tYeU8ZY+W6uvpR1qT1GrsfQOZ9neQtESkiyYJWYmqIynbNxBCyCHDSZIQQgw4SRJCiEH7UmmQTpgXsoSZ1iC9FMZCaoUBvQs1SbHsaRoOdYp63DKgSWLrVhW/BqlnLtHpcVVc22Vkx30651xe6X+LpLYWeb1Q9XlLHTLL7GvahZa7pYxJ9drN6n1lVbWQbgNV1lRL4azS57A7wTJ6tc6YBDTJWabfOXlWJai5mN5ZCC03EF7qdmb6gjZ26nPe3Na6KLZElu9+OrDjPp3zn7dl/WulIe5Hp2vad78xl4cZq+kfqxTrqEkSQsihwkmSEEIM2qclZuhu166Flz6GKW8qvMQex/tSrhqNORuXxjrruNpdw6o/MVQFSgbj2giE5TjnXA63WaZPYnpgBamX2ay9u70N1eMzUREe3bwUqnWnqajOE3CD8SxkBaESzn8XKnFvpHX6YKgKEFYUWlxc3FteWtKVq6sH6H13p/nc5XmcuOD+yh6O6rTEdKArcbsYz1mWmQ+7jBHKQMY+tmN4iHmJX+KE5ILKmkNawC9JQggx4CRJCCEGnCQJIcQgqr5YdZIIIeRLAH5JEkKIASdJQggx4CRJCCEGreMk//rv/kXZhYiNw3YNUyidJktrYYrY93zbNyj7D/7s443jZAW2kND2TNgz6OD47B95grJ//Z03KXt3Z6de3t1R6zY3NsHe2Fve2tLr3vraVzrk4mc9T9lT0emvLPR5lnBNmbi32BHw5g/+sbIf/ZhvVfZEXFME0Y39XtpoD2DdB2/+qLJ/4Hsfo+xCnHMB5z8a6JTO8agunXbqPF127H9dd6Oy/+fP/5SyLzj//L3l5eWjat3C4hFlb+7U921zW9/jpz37cmXf8OY3Ok0dd3fs2DG15oEPepCyT506tbc8XlhQ65aOHXdIAe+W/R8CXiE9sajXpdAmI4f0ycP6r4feSJe+m0E5O6slg9fNU6zHbY+e0Pfu3F26nF2XMmtqPWx63v3OdyH4JUkIIQacJAkhxICTJCGEGLTWJDPI3ZZtG8sCS5YZGkFAP8D2BVJ5SbAsE9Rpr2S5rJ59aYM+lD9TSeVwXCh1JnO7e32t0cxj5cRJZUtNssi0Xlbk2paaZA6aJLK4rPWzSLSZmM10DnVWQr69yG8OdPf0cqFlvnkF70IMpeQiV1/DNLNzqrH82XBca37HT55S606eugDOsZy7PI+Hf+0jlC3f3/F4rNatrBzV5zSsc7vTNNzKA5Fv9JdDwLLMo8ac6hj7KkipMJCKHoE+qzvxtm8TsZ+bzC9JQggx4CRJCCEG+3a35ScslkbzO/HVi6ECT0bQg4thbQKltJwo8RUaZ9Drw1/qY8UxlEqD0mlpv3ax+kPtjs1j5bh2DWfT2t3GsJ48A1uFAGlXHFk6ot1t6fnmm/qOTHa29baiijlWHkfQ3Y5ENfKoxOrcUGlevCtYfs8j0u7raFyXSjsB7vYDH/IwZWdF/Tyz0naDv/prvk7Z8v2NY33felBGLknq9XGrCt9WabRml/JLESx1h7dH2nHI3cbfuuhCgPMNhhOVofqMAfglSQghBpwkCSHEgJMkIYQYtNYkMbxBtmRAsSEG/aBQqUq2PpBCmIDukKdBjTIWc35U2fN/P9VpXJGrry+J9G1JIBSll9YaZb+n9cp5YApdNqt1zBxCgHw7a1yHnDipw2AGIjxlYVGnT25vbSk7E2FJMcZhAYOxTgEsZvW+OYQa5V54mGirMLGvZwtS3rZFK4gM8ltj6HDZE20WYmdrkgNI6ZNhap5G7r3rrnHb+TRv9cXSIA+WsmhdNR4XOlyqexm6e6hJylYtsK9nixTGwCgtRiaEECLhJEkIIQacJAkhxKC1JtmDND8VJxlrT79IsESSjKm0VQGMQ5OxYyXoFiXM8bGTWpKtQ/USHScZiZi8JNbnn0KcZK9X62r9FmmJniYp0vFkfOI8OxdpinkgrvA8SM1bXKq1w20o6bYF5d92RNxkkdnpj+PFo8re3RYxiZmOk8wLTLuUmqQ9zuaWjuXcEPYEyvF5r5V4VTCdFUkwXc4g0Ly08xG6lUo7HO7Nji0qA9DTBnFrHQVt4beNrbevKtS9I8NmS1lCCDlUOEkSQohBa3cbK3hYlYHxg1Zva+e8+SEWtV1hyA9WBRKhK6E0JwxTSpSJx/VOUiyH/53p95pd+wJCV4pEu9tJImUAW0IYGimSGHpV5FAlWjyXLHDzErieOJbXgO+JA7v+Q57b70IG0oOSKTC0CPbVKX729bRLJ2xCjhw+TnWgsb7c6OIGH9Z9634cfkkSQogBJ0lCCDHgJEkIIQZRdW/GAxBCyJc4/JIkhBADTpKEEGLASZIQQgw4SRJCiAEnSUIIMeAkSQghBpwkCSHEgJMkIYQYcJIkhBCD/w9v+LaszFUyKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure for displaying the original image\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]  # Select a random image from the training data\n",
    "plt.imshow(image.astype(\"uint8\"))  # Display the image\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Resize the image to the specified image size\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "\n",
    "# Extract patches from the resized image using the custom Patches layer\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "\n",
    "# Print information about the patches and their sizes\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "# Create a figure for displaying the extracted patches\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21336333",
   "metadata": {},
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01055a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a custom layer called \"PatchEncoder\" for encoding patches\n",
    "@keras.saving.register_keras_serializable()\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_patches (int): The number of patches extracted from an image.\n",
    "            projection_dim (int): The dimensionality of the projected vector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of patches and projection dimension are initialized as attributes\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # Define a dense layer for linear transformation (projection) of each patch\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        \n",
    "        # Define an embedding layer for adding learnable position embeddings\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch (tf.Tensor): A patch (vector) extracted from an image.\n",
    "        \n",
    "        Returns:\n",
    "            encoded (tf.Tensor): The encoded representation of the input patch.\n",
    "        \"\"\"\n",
    "        # Generate positional embeddings based on the number of patches\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        \n",
    "        # Apply linear transformation (projection) to the input patch\n",
    "        projected_patch = self.projection(patch)\n",
    "        \n",
    "        # Add learnable position embeddings to the projected vector\n",
    "        encoded = projected_patch + self.position_embedding(positions)\n",
    "        \n",
    "        # Return the encoded representation of the input patch\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7a9c3",
   "metadata": {},
   "source": [
    "## Build the ViT model\n",
    "The ViT model consists of multiple Transformer blocks, which use the layers.MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a [batch_size, num_patches, projection_dim] tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with layers.Flatten() and used as the image representation input to the classifier head. Note that the layers.GlobalAveragePooling1D layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbcfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    # Define the input layer with the specified input shape (e.g., image dimensions)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Augment the input data using the data augmentation pipeline defined earlier\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    # Create patches from the augmented data using the custom Patches layer\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    \n",
    "    # Encode the patches using the custom PatchEncoder layer\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block for feature extraction.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1 is applied to the encoded patches.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        \n",
    "        # Create a multi-head attention layer to capture dependencies between patches.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip connection 1 adds the attention output to the encoded patches.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2 is applied to the combined output.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # MLP (Multi-Layer Perceptron) processes the output to capture complex features.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        \n",
    "        # Skip connection 2 adds the MLP output to the previous output.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a representation tensor by applying layer normalization and flattening.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    \n",
    "    # Apply dropout regularization to the representation tensor for improved generalization.\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add another MLP to process the representation and extract high-level features.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "    # Classify the extracted features into the specified number of classes.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    \n",
    "    # Create the Keras model with the defined inputs and outputs.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c32d8",
   "metadata": {},
   "source": [
    "## Explanation of above code ðŸ‘†\n",
    "The function starts by defining the input layer with the specified input_shape.\n",
    "The input data is augmented using the data augmentation pipeline defined earlier to increase the diversity of the training data.\n",
    "\n",
    "Patches are created from the augmented data using the custom Patches layer.\n",
    "\n",
    "The patches are encoded using the custom PatchEncoder layer, which adds positional embeddings to each patch.\n",
    "\n",
    "A stack of Transformer blocks is applied to the encoded patches to extract hierarchical features. This includes multi-head self-attention and MLP layers.\n",
    "\n",
    "The output of the Transformer blocks is normalized, flattened, and subjected to dropout regularization for improved generalization.\n",
    "\n",
    "Another MLP is applied to the representation to further capture high-level features.\n",
    "\n",
    "Finally, the extracted features are classified into the specified number of classes using a dense layer, and the Keras model is created with the defined inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8c0a4",
   "metadata": {},
   "source": [
    "## Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5470c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Define the optimizer using AdamW with specified learning rate and weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Compile the model with loss and evaluation metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Define a checkpoint to save the best model weights during training\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    # Train the model and store training history\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,  # Use 10% of the training data as validation\n",
    "        callbacks=[checkpoint_callback],  # Save the best model during training\n",
    "    )\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "    # Print test accuracy and top-5 accuracy\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Create a Vision Transformer (ViT) classifier model\n",
    "vit_classifier = create_vit_classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379033ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "176/176 [==============================] - 902s 5s/step - loss: 4.4842 - accuracy: 0.0444 - top-5-accuracy: 0.1572 - val_loss: 3.9307 - val_accuracy: 0.1004 - val_top-5-accuracy: 0.3062\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 838s 5s/step - loss: 3.9633 - accuracy: 0.0910 - top-5-accuracy: 0.2850 - val_loss: 3.6200 - val_accuracy: 0.1568 - val_top-5-accuracy: 0.3958\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 861s 5s/step - loss: 3.7021 - accuracy: 0.1310 - top-5-accuracy: 0.3647 - val_loss: 3.3343 - val_accuracy: 0.1970 - val_top-5-accuracy: 0.4772\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 3.5056 - accuracy: 0.1620 - top-5-accuracy: 0.4196 - val_loss: 3.1996 - val_accuracy: 0.2232 - val_top-5-accuracy: 0.5106\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 889s 5s/step - loss: 3.3591 - accuracy: 0.1868 - top-5-accuracy: 0.4612 - val_loss: 3.0827 - val_accuracy: 0.2484 - val_top-5-accuracy: 0.5378\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 3.2138 - accuracy: 0.2144 - top-5-accuracy: 0.4993 - val_loss: 2.9775 - val_accuracy: 0.2618 - val_top-5-accuracy: 0.5612\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 890s 5s/step - loss: 3.0895 - accuracy: 0.2382 - top-5-accuracy: 0.5352 - val_loss: 2.8295 - val_accuracy: 0.2872 - val_top-5-accuracy: 0.5982\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 890s 5s/step - loss: 2.9604 - accuracy: 0.2616 - top-5-accuracy: 0.5673 - val_loss: 2.7140 - val_accuracy: 0.3112 - val_top-5-accuracy: 0.6202\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 884s 5s/step - loss: 2.8533 - accuracy: 0.2846 - top-5-accuracy: 0.5921 - val_loss: 2.6336 - val_accuracy: 0.3302 - val_top-5-accuracy: 0.6420\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 873s 5s/step - loss: 2.7437 - accuracy: 0.3066 - top-5-accuracy: 0.6188 - val_loss: 2.5337 - val_accuracy: 0.3596 - val_top-5-accuracy: 0.6616\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 923s 5s/step - loss: 2.6605 - accuracy: 0.3247 - top-5-accuracy: 0.6368 - val_loss: 2.4576 - val_accuracy: 0.3714 - val_top-5-accuracy: 0.6846\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 906s 5s/step - loss: 2.5741 - accuracy: 0.3430 - top-5-accuracy: 0.6540 - val_loss: 2.3726 - val_accuracy: 0.3964 - val_top-5-accuracy: 0.6982\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 2.5185 - accuracy: 0.3525 - top-5-accuracy: 0.6693 - val_loss: 2.3495 - val_accuracy: 0.4010 - val_top-5-accuracy: 0.6960\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 2.4251 - accuracy: 0.3745 - top-5-accuracy: 0.6872 - val_loss: 2.2999 - val_accuracy: 0.4052 - val_top-5-accuracy: 0.7062\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 888s 5s/step - loss: 2.3629 - accuracy: 0.3844 - top-5-accuracy: 0.7017 - val_loss: 2.2486 - val_accuracy: 0.4158 - val_top-5-accuracy: 0.7194\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 877s 5s/step - loss: 2.2924 - accuracy: 0.4018 - top-5-accuracy: 0.7176 - val_loss: 2.2525 - val_accuracy: 0.4152 - val_top-5-accuracy: 0.7116\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 2.2272 - accuracy: 0.4136 - top-5-accuracy: 0.7277 - val_loss: 2.2643 - val_accuracy: 0.4144 - val_top-5-accuracy: 0.7142\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 899s 5s/step - loss: 2.1828 - accuracy: 0.4243 - top-5-accuracy: 0.7383 - val_loss: 2.1485 - val_accuracy: 0.4360 - val_top-5-accuracy: 0.7336\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 1042s 6s/step - loss: 2.1240 - accuracy: 0.4364 - top-5-accuracy: 0.7514 - val_loss: 2.1330 - val_accuracy: 0.4404 - val_top-5-accuracy: 0.7406\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 896s 5s/step - loss: 2.0666 - accuracy: 0.4492 - top-5-accuracy: 0.7589 - val_loss: 2.1085 - val_accuracy: 0.4464 - val_top-5-accuracy: 0.7386\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 878s 5s/step - loss: 2.0138 - accuracy: 0.4597 - top-5-accuracy: 0.7717 - val_loss: 2.1030 - val_accuracy: 0.4518 - val_top-5-accuracy: 0.7454\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 880s 5s/step - loss: 1.9581 - accuracy: 0.4702 - top-5-accuracy: 0.7841 - val_loss: 2.0024 - val_accuracy: 0.4730 - val_top-5-accuracy: 0.7640\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 870s 5s/step - loss: 1.9070 - accuracy: 0.4856 - top-5-accuracy: 0.7924 - val_loss: 2.0200 - val_accuracy: 0.4718 - val_top-5-accuracy: 0.7636\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 869s 5s/step - loss: 1.8765 - accuracy: 0.4887 - top-5-accuracy: 0.7984 - val_loss: 1.9842 - val_accuracy: 0.4808 - val_top-5-accuracy: 0.7692\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 876s 5s/step - loss: 1.8217 - accuracy: 0.5052 - top-5-accuracy: 0.8072 - val_loss: 2.0364 - val_accuracy: 0.4682 - val_top-5-accuracy: 0.7616\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 865s 5s/step - loss: 1.7939 - accuracy: 0.5089 - top-5-accuracy: 0.8136 - val_loss: 1.9814 - val_accuracy: 0.4806 - val_top-5-accuracy: 0.7698\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 875s 5s/step - loss: 1.7516 - accuracy: 0.5203 - top-5-accuracy: 0.8214 - val_loss: 1.9597 - val_accuracy: 0.4914 - val_top-5-accuracy: 0.7712\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 867s 5s/step - loss: 1.7036 - accuracy: 0.5296 - top-5-accuracy: 0.8296 - val_loss: 1.9347 - val_accuracy: 0.4936 - val_top-5-accuracy: 0.7828\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 873s 5s/step - loss: 1.6728 - accuracy: 0.5394 - top-5-accuracy: 0.8349 - val_loss: 1.9261 - val_accuracy: 0.5000 - val_top-5-accuracy: 0.7812\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 869s 5s/step - loss: 1.6460 - accuracy: 0.5427 - top-5-accuracy: 0.8405 - val_loss: 1.8963 - val_accuracy: 0.5010 - val_top-5-accuracy: 0.7880\n",
      "Epoch 31/100\n",
      "176/176 [==============================] - 884s 5s/step - loss: 1.6056 - accuracy: 0.5535 - top-5-accuracy: 0.8460 - val_loss: 1.9378 - val_accuracy: 0.4926 - val_top-5-accuracy: 0.7782\n",
      "Epoch 32/100\n",
      "176/176 [==============================] - 878s 5s/step - loss: 1.5786 - accuracy: 0.5592 - top-5-accuracy: 0.8501 - val_loss: 1.8893 - val_accuracy: 0.5110 - val_top-5-accuracy: 0.7928\n",
      "Epoch 33/100\n",
      "176/176 [==============================] - 881s 5s/step - loss: 1.5441 - accuracy: 0.5650 - top-5-accuracy: 0.8581 - val_loss: 1.8764 - val_accuracy: 0.5154 - val_top-5-accuracy: 0.7936\n",
      "Epoch 34/100\n",
      "176/176 [==============================] - 869s 5s/step - loss: 1.5147 - accuracy: 0.5719 - top-5-accuracy: 0.8632 - val_loss: 1.8788 - val_accuracy: 0.5196 - val_top-5-accuracy: 0.7982\n",
      "Epoch 35/100\n",
      "176/176 [==============================] - 884s 5s/step - loss: 1.4789 - accuracy: 0.5821 - top-5-accuracy: 0.8674 - val_loss: 1.8858 - val_accuracy: 0.5136 - val_top-5-accuracy: 0.7914\n",
      "Epoch 36/100\n",
      "176/176 [==============================] - 870s 5s/step - loss: 1.4590 - accuracy: 0.5866 - top-5-accuracy: 0.8711 - val_loss: 1.8588 - val_accuracy: 0.5120 - val_top-5-accuracy: 0.7912\n",
      "Epoch 37/100\n",
      "176/176 [==============================] - 877s 5s/step - loss: 1.4227 - accuracy: 0.5988 - top-5-accuracy: 0.8772 - val_loss: 1.8758 - val_accuracy: 0.5148 - val_top-5-accuracy: 0.7924\n",
      "Epoch 38/100\n",
      "176/176 [==============================] - 878s 5s/step - loss: 1.3988 - accuracy: 0.6022 - top-5-accuracy: 0.8794 - val_loss: 1.8771 - val_accuracy: 0.5186 - val_top-5-accuracy: 0.7992\n",
      "Epoch 39/100\n",
      "176/176 [==============================] - 869s 5s/step - loss: 1.3720 - accuracy: 0.6072 - top-5-accuracy: 0.8831 - val_loss: 1.8577 - val_accuracy: 0.5204 - val_top-5-accuracy: 0.7978\n",
      "Epoch 40/100\n",
      "176/176 [==============================] - 872s 5s/step - loss: 1.3657 - accuracy: 0.6102 - top-5-accuracy: 0.8856 - val_loss: 1.8913 - val_accuracy: 0.5168 - val_top-5-accuracy: 0.7922\n",
      "Epoch 41/100\n",
      "176/176 [==============================] - 877s 5s/step - loss: 1.3423 - accuracy: 0.6157 - top-5-accuracy: 0.8886 - val_loss: 1.8350 - val_accuracy: 0.5318 - val_top-5-accuracy: 0.7984\n",
      "Epoch 42/100\n",
      "176/176 [==============================] - 876s 5s/step - loss: 1.3221 - accuracy: 0.6198 - top-5-accuracy: 0.8932 - val_loss: 1.8545 - val_accuracy: 0.5250 - val_top-5-accuracy: 0.8018\n",
      "Epoch 43/100\n",
      "176/176 [==============================] - 868s 5s/step - loss: 1.2886 - accuracy: 0.6298 - top-5-accuracy: 0.8975 - val_loss: 1.9030 - val_accuracy: 0.5146 - val_top-5-accuracy: 0.7936\n",
      "Epoch 44/100\n",
      "176/176 [==============================] - 867s 5s/step - loss: 1.2753 - accuracy: 0.6342 - top-5-accuracy: 0.8986 - val_loss: 1.8514 - val_accuracy: 0.5240 - val_top-5-accuracy: 0.8016\n",
      "Epoch 45/100\n",
      "176/176 [==============================] - 872s 5s/step - loss: 1.2686 - accuracy: 0.6325 - top-5-accuracy: 0.9014 - val_loss: 1.8351 - val_accuracy: 0.5296 - val_top-5-accuracy: 0.8038\n",
      "Epoch 46/100\n",
      "176/176 [==============================] - 871s 5s/step - loss: 1.2301 - accuracy: 0.6422 - top-5-accuracy: 0.9070 - val_loss: 1.8741 - val_accuracy: 0.5258 - val_top-5-accuracy: 0.8004\n",
      "Epoch 47/100\n",
      "176/176 [==============================] - 873s 5s/step - loss: 1.2114 - accuracy: 0.6494 - top-5-accuracy: 0.9087 - val_loss: 1.8707 - val_accuracy: 0.5188 - val_top-5-accuracy: 0.7946\n",
      "Epoch 48/100\n",
      "176/176 [==============================] - 874s 5s/step - loss: 1.2115 - accuracy: 0.6492 - top-5-accuracy: 0.9088 - val_loss: 1.8580 - val_accuracy: 0.5330 - val_top-5-accuracy: 0.8064\n",
      "Epoch 49/100\n",
      "176/176 [==============================] - 866s 5s/step - loss: 1.1737 - accuracy: 0.6571 - top-5-accuracy: 0.9128 - val_loss: 1.8308 - val_accuracy: 0.5304 - val_top-5-accuracy: 0.8062\n",
      "Epoch 50/100\n",
      "176/176 [==============================] - 851s 5s/step - loss: 1.1657 - accuracy: 0.6598 - top-5-accuracy: 0.9144 - val_loss: 1.8497 - val_accuracy: 0.5344 - val_top-5-accuracy: 0.8064\n",
      "Epoch 51/100\n",
      "176/176 [==============================] - 784s 4s/step - loss: 1.1442 - accuracy: 0.6667 - top-5-accuracy: 0.9166 - val_loss: 1.8481 - val_accuracy: 0.5362 - val_top-5-accuracy: 0.8094\n",
      "Epoch 52/100\n",
      "176/176 [==============================] - 867s 5s/step - loss: 1.1312 - accuracy: 0.6711 - top-5-accuracy: 0.9198 - val_loss: 1.8430 - val_accuracy: 0.5474 - val_top-5-accuracy: 0.8076\n",
      "Epoch 53/100\n",
      "176/176 [==============================] - 868s 5s/step - loss: 1.1093 - accuracy: 0.6765 - top-5-accuracy: 0.9221 - val_loss: 1.8445 - val_accuracy: 0.5434 - val_top-5-accuracy: 0.8086\n",
      "Epoch 54/100\n",
      "176/176 [==============================] - 866s 5s/step - loss: 1.1113 - accuracy: 0.6751 - top-5-accuracy: 0.9218 - val_loss: 1.8516 - val_accuracy: 0.5378 - val_top-5-accuracy: 0.8098\n",
      "Epoch 55/100\n",
      "176/176 [==============================] - 868s 5s/step - loss: 1.0957 - accuracy: 0.6770 - top-5-accuracy: 0.9255 - val_loss: 1.8514 - val_accuracy: 0.5342 - val_top-5-accuracy: 0.8036\n",
      "Epoch 56/100\n",
      "176/176 [==============================] - 863s 5s/step - loss: 1.0844 - accuracy: 0.6813 - top-5-accuracy: 0.9250 - val_loss: 1.8577 - val_accuracy: 0.5330 - val_top-5-accuracy: 0.8090\n",
      "Epoch 57/100\n",
      "176/176 [==============================] - 863s 5s/step - loss: 1.0777 - accuracy: 0.6826 - top-5-accuracy: 0.9264 - val_loss: 1.8841 - val_accuracy: 0.5330 - val_top-5-accuracy: 0.8030\n",
      "Epoch 58/100\n",
      "176/176 [==============================] - 871s 5s/step - loss: 1.0595 - accuracy: 0.6860 - top-5-accuracy: 0.9295 - val_loss: 1.8649 - val_accuracy: 0.5340 - val_top-5-accuracy: 0.8052\n",
      "Epoch 59/100\n",
      "176/176 [==============================] - 871s 5s/step - loss: 1.0563 - accuracy: 0.6877 - top-5-accuracy: 0.9295 - val_loss: 1.8616 - val_accuracy: 0.5386 - val_top-5-accuracy: 0.8034\n",
      "Epoch 60/100\n",
      "176/176 [==============================] - 862s 5s/step - loss: 1.0320 - accuracy: 0.6953 - top-5-accuracy: 0.9328 - val_loss: 1.8669 - val_accuracy: 0.5468 - val_top-5-accuracy: 0.8052\n",
      "Epoch 61/100\n",
      "176/176 [==============================] - 867s 5s/step - loss: 1.0140 - accuracy: 0.7019 - top-5-accuracy: 0.9335 - val_loss: 1.8519 - val_accuracy: 0.5426 - val_top-5-accuracy: 0.8150\n",
      "Epoch 62/100\n",
      "176/176 [==============================] - 864s 5s/step - loss: 1.0084 - accuracy: 0.7024 - top-5-accuracy: 0.9372 - val_loss: 1.8832 - val_accuracy: 0.5270 - val_top-5-accuracy: 0.8082\n",
      "Epoch 63/100\n",
      "176/176 [==============================] - 862s 5s/step - loss: 0.9996 - accuracy: 0.7050 - top-5-accuracy: 0.9356 - val_loss: 1.8563 - val_accuracy: 0.5404 - val_top-5-accuracy: 0.8080\n",
      "Epoch 64/100\n",
      "176/176 [==============================] - 865s 5s/step - loss: 0.9880 - accuracy: 0.7063 - top-5-accuracy: 0.9380 - val_loss: 1.8624 - val_accuracy: 0.5390 - val_top-5-accuracy: 0.8068\n",
      "Epoch 65/100\n",
      "176/176 [==============================] - 863s 5s/step - loss: 0.9820 - accuracy: 0.7078 - top-5-accuracy: 0.9391 - val_loss: 1.8679 - val_accuracy: 0.5418 - val_top-5-accuracy: 0.8152\n",
      "Epoch 66/100\n",
      "176/176 [==============================] - 862s 5s/step - loss: 0.9794 - accuracy: 0.7099 - top-5-accuracy: 0.9401 - val_loss: 1.8841 - val_accuracy: 0.5414 - val_top-5-accuracy: 0.8056\n",
      "Epoch 67/100\n",
      "176/176 [==============================] - 864s 5s/step - loss: 0.9732 - accuracy: 0.7095 - top-5-accuracy: 0.9399 - val_loss: 1.8593 - val_accuracy: 0.5442 - val_top-5-accuracy: 0.8128\n",
      "Epoch 68/100\n",
      "176/176 [==============================] - 853s 5s/step - loss: 0.9447 - accuracy: 0.7196 - top-5-accuracy: 0.9431 - val_loss: 1.8670 - val_accuracy: 0.5402 - val_top-5-accuracy: 0.8112\n",
      "Epoch 69/100\n",
      "176/176 [==============================] - 860s 5s/step - loss: 0.9530 - accuracy: 0.7166 - top-5-accuracy: 0.9425 - val_loss: 1.8926 - val_accuracy: 0.5362 - val_top-5-accuracy: 0.8092\n",
      "Epoch 70/100\n",
      "176/176 [==============================] - 865s 5s/step - loss: 0.9415 - accuracy: 0.7215 - top-5-accuracy: 0.9443 - val_loss: 1.8544 - val_accuracy: 0.5430 - val_top-5-accuracy: 0.8116\n",
      "Epoch 71/100\n",
      "176/176 [==============================] - 871s 5s/step - loss: 0.9314 - accuracy: 0.7231 - top-5-accuracy: 0.9447 - val_loss: 1.8853 - val_accuracy: 0.5412 - val_top-5-accuracy: 0.8112\n",
      "Epoch 72/100\n",
      "176/176 [==============================] - 865s 5s/step - loss: 0.9164 - accuracy: 0.7280 - top-5-accuracy: 0.9472 - val_loss: 1.9024 - val_accuracy: 0.5392 - val_top-5-accuracy: 0.8038\n",
      "Epoch 73/100\n",
      "176/176 [==============================] - 863s 5s/step - loss: 0.9041 - accuracy: 0.7324 - top-5-accuracy: 0.9476 - val_loss: 1.8992 - val_accuracy: 0.5436 - val_top-5-accuracy: 0.8106\n",
      "Epoch 74/100\n",
      "176/176 [==============================] - 873s 5s/step - loss: 0.9015 - accuracy: 0.7306 - top-5-accuracy: 0.9475 - val_loss: 1.8782 - val_accuracy: 0.5328 - val_top-5-accuracy: 0.8104\n",
      "Epoch 75/100\n",
      "176/176 [==============================] - 888s 5s/step - loss: 0.9060 - accuracy: 0.7292 - top-5-accuracy: 0.9480 - val_loss: 1.9379 - val_accuracy: 0.5338 - val_top-5-accuracy: 0.8048\n",
      "Epoch 76/100\n",
      "176/176 [==============================] - 876s 5s/step - loss: 0.8924 - accuracy: 0.7337 - top-5-accuracy: 0.9491 - val_loss: 1.8575 - val_accuracy: 0.5452 - val_top-5-accuracy: 0.8164\n",
      "Epoch 77/100\n",
      "176/176 [==============================] - 861s 5s/step - loss: 0.8868 - accuracy: 0.7344 - top-5-accuracy: 0.9503 - val_loss: 1.8688 - val_accuracy: 0.5396 - val_top-5-accuracy: 0.8122\n",
      "Epoch 78/100\n",
      "176/176 [==============================] - 868s 5s/step - loss: 0.8781 - accuracy: 0.7397 - top-5-accuracy: 0.9513 - val_loss: 1.8875 - val_accuracy: 0.5492 - val_top-5-accuracy: 0.8098\n",
      "Epoch 79/100\n",
      "176/176 [==============================] - 870s 5s/step - loss: 0.8594 - accuracy: 0.7418 - top-5-accuracy: 0.9524 - val_loss: 1.9023 - val_accuracy: 0.5456 - val_top-5-accuracy: 0.8140\n",
      "Epoch 80/100\n",
      "176/176 [==============================] - 890s 5s/step - loss: 0.8594 - accuracy: 0.7416 - top-5-accuracy: 0.9536 - val_loss: 1.8683 - val_accuracy: 0.5532 - val_top-5-accuracy: 0.8204\n",
      "Epoch 81/100\n",
      "176/176 [==============================] - 886s 5s/step - loss: 0.8703 - accuracy: 0.7417 - top-5-accuracy: 0.9516 - val_loss: 1.9186 - val_accuracy: 0.5402 - val_top-5-accuracy: 0.8056\n",
      "Epoch 82/100\n",
      "176/176 [==============================] - 878s 5s/step - loss: 0.8577 - accuracy: 0.7436 - top-5-accuracy: 0.9520 - val_loss: 1.9242 - val_accuracy: 0.5480 - val_top-5-accuracy: 0.8120\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 875s 5s/step - loss: 0.8638 - accuracy: 0.7416 - top-5-accuracy: 0.9522 - val_loss: 1.9068 - val_accuracy: 0.5406 - val_top-5-accuracy: 0.8080\n",
      "Epoch 84/100\n",
      "176/176 [==============================] - 889s 5s/step - loss: 0.8320 - accuracy: 0.7507 - top-5-accuracy: 0.9552 - val_loss: 1.8992 - val_accuracy: 0.5388 - val_top-5-accuracy: 0.8072\n",
      "Epoch 85/100\n",
      "176/176 [==============================] - 883s 5s/step - loss: 0.8398 - accuracy: 0.7492 - top-5-accuracy: 0.9550 - val_loss: 1.9402 - val_accuracy: 0.5448 - val_top-5-accuracy: 0.8178\n",
      "Epoch 86/100\n",
      "176/176 [==============================] - 888s 5s/step - loss: 0.8468 - accuracy: 0.7478 - top-5-accuracy: 0.9548 - val_loss: 1.8743 - val_accuracy: 0.5462 - val_top-5-accuracy: 0.8168\n",
      "Epoch 87/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 0.8218 - accuracy: 0.7524 - top-5-accuracy: 0.9564 - val_loss: 1.9064 - val_accuracy: 0.5490 - val_top-5-accuracy: 0.8156\n",
      "Epoch 88/100\n",
      "176/176 [==============================] - 882s 5s/step - loss: 0.8241 - accuracy: 0.7529 - top-5-accuracy: 0.9567 - val_loss: 1.9126 - val_accuracy: 0.5460 - val_top-5-accuracy: 0.8172\n",
      "Epoch 89/100\n",
      "176/176 [==============================] - 884s 5s/step - loss: 0.8187 - accuracy: 0.7541 - top-5-accuracy: 0.9571 - val_loss: 1.9225 - val_accuracy: 0.5426 - val_top-5-accuracy: 0.8148\n",
      "Epoch 90/100\n",
      "176/176 [==============================] - 894s 5s/step - loss: 0.8202 - accuracy: 0.7530 - top-5-accuracy: 0.9573 - val_loss: 1.8946 - val_accuracy: 0.5516 - val_top-5-accuracy: 0.8112\n",
      "Epoch 91/100\n",
      "176/176 [==============================] - 900s 5s/step - loss: 0.8299 - accuracy: 0.7515 - top-5-accuracy: 0.9559 - val_loss: 1.9122 - val_accuracy: 0.5490 - val_top-5-accuracy: 0.8080\n",
      "Epoch 92/100\n",
      "176/176 [==============================] - 1023s 6s/step - loss: 0.7949 - accuracy: 0.7607 - top-5-accuracy: 0.9593 - val_loss: 1.9202 - val_accuracy: 0.5418 - val_top-5-accuracy: 0.8036\n",
      "Epoch 93/100\n",
      "176/176 [==============================] - 968s 5s/step - loss: 0.8034 - accuracy: 0.7585 - top-5-accuracy: 0.9579 - val_loss: 1.8991 - val_accuracy: 0.5442 - val_top-5-accuracy: 0.8086\n",
      "Epoch 94/100\n",
      "176/176 [==============================] - 885s 5s/step - loss: 0.7970 - accuracy: 0.7588 - top-5-accuracy: 0.9586 - val_loss: 1.8925 - val_accuracy: 0.5516 - val_top-5-accuracy: 0.8142\n",
      "Epoch 95/100\n",
      "176/176 [==============================] - 877s 5s/step - loss: 0.8049 - accuracy: 0.7595 - top-5-accuracy: 0.9570 - val_loss: 1.9482 - val_accuracy: 0.5482 - val_top-5-accuracy: 0.8080\n",
      "Epoch 96/100\n",
      "176/176 [==============================] - 891s 5s/step - loss: 0.7777 - accuracy: 0.7672 - top-5-accuracy: 0.9612 - val_loss: 1.9393 - val_accuracy: 0.5422 - val_top-5-accuracy: 0.8004\n",
      "Epoch 97/100\n",
      "176/176 [==============================] - 895s 5s/step - loss: 0.7960 - accuracy: 0.7612 - top-5-accuracy: 0.9598 - val_loss: 1.8928 - val_accuracy: 0.5480 - val_top-5-accuracy: 0.8114\n",
      "Epoch 98/100\n",
      "176/176 [==============================] - 898s 5s/step - loss: 0.7888 - accuracy: 0.7654 - top-5-accuracy: 0.9595 - val_loss: 1.9096 - val_accuracy: 0.5450 - val_top-5-accuracy: 0.8096\n",
      "Epoch 99/100\n",
      "176/176 [==============================] - 898s 5s/step - loss: 0.7768 - accuracy: 0.7682 - top-5-accuracy: 0.9603 - val_loss: 1.9603 - val_accuracy: 0.5412 - val_top-5-accuracy: 0.8088\n",
      "Epoch 100/100\n",
      "176/176 [==============================] - 898s 5s/step - loss: 0.7763 - accuracy: 0.7668 - top-5-accuracy: 0.9609 - val_loss: 1.9728 - val_accuracy: 0.5342 - val_top-5-accuracy: 0.8038\n",
      "313/313 [==============================] - 68s 217ms/step - loss: 1.8641 - accuracy: 0.5496 - top-5-accuracy: 0.8133\n",
      "Test accuracy: 54.96%\n",
      "Test top 5 accuracy: 81.33%\n"
     ]
    }
   ],
   "source": [
    "# Run an experiment to train and evaluate the ViT model\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cefc4051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nTo Load the saved ViT-based image classifier model:\\n\\nloaded_model = keras.models.load_model('vit_classifier.keras')\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the entire ViT-based image classifier model to a file with the '.keras' extension\n",
    "\n",
    "vit_classifier.save('vit_classifier.keras')\n",
    "\n",
    "''' \n",
    "To Load the saved ViT-based image classifier model:\n",
    "\n",
    "loaded_model = keras.models.load_model('vit_classifier.keras')\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
